{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as n\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / np.sum(e_x, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implements a single forward step of the RNN-cell\n",
    "\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        ba --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    Returns:\n",
    "    a_next -- next hidden state, of shape (n_a, m)\n",
    "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "\n",
    "    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)  # (n_a, m)\n",
    "    yt_pred = softmax(np.dot(Wya, a_next) + by)                   # (n_y, m)\n",
    "    cache = (a_next, a_prev, xt, parameters)\n",
    "\n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
    "    a0 -- Initial hidden state, of shape (n_a, m)\n",
    "    parameters -- python dictionary containing parameters matrices\n",
    "\n",
    "    Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "\n",
    "    a = np.zeros((n_a, m, T_x))\n",
    "    y_pred = np.zeros((n_y, m, T_x))\n",
    "\n",
    "    a_next = a0\n",
    "    for t in range(T_x):\n",
    "        a_next, yt_pred, cache = rnn_cell_forward(x[:, :, t], a_next, parameters)\n",
    "        a[:, :, t] = a_next\n",
    "        y_pred[:, :, t] = yt_pred\n",
    "        caches.append(cache)\n",
    "\n",
    "    caches = (caches, x)\n",
    "\n",
    "    return a, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_backward(da_next, cache):\n",
    "    \"\"\"\n",
    "    Implements the backward pass for a single RNN cell (1 time-step).\n",
    "\n",
    "    Arguments:\n",
    "    da_next -- Gradients of the next hidden state, of shape (n_a, m)\n",
    "               the gradient of loss w.r.t. the current hidden state (from the future)\n",
    "    cache -- Tuple of values (a_next, a_prev, xt, parameters) from forward pass\n",
    "\n",
    "    Returns:\n",
    "    gradients -- Dictionary containing:\n",
    "        dxt -- Gradient of input data at time-step t, of shape (n_x, m)\n",
    "        da_prev -- Gradient w.r.t. previous hidden state, of shape (n_a, m)\n",
    "        dWax -- Gradient w.r.t. input weight matrix, shape (n_a, n_x)\n",
    "        dWaa -- Gradient w.r.t. hidden state weight matrix, shape (n_a, n_a)\n",
    "        dba -- Gradient w.r.t. bias vector, shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    # Shapes from forward\n",
    "    # xt     → (n_x, m)\n",
    "    # a_prev → (n_a, m)\n",
    "    # Wax    → (n_a, n_x)\n",
    "    # Waa    → (n_a, n_a)\n",
    "    # Wya    → (n_y, n_a)\n",
    "    # ba     → (n_a, 1)\n",
    "    # by     → (n_y, 1)\n",
    "\n",
    "    (a_next, a_prev, xt, parameters) = cache\n",
    "\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "\n",
    "    # Derivative through tanh activation\n",
    "    dtanh = (1 - a_next**2) * da_next     # (n_a, m)\n",
    "\n",
    "    # Gradients\n",
    "    dxt    = np.dot(Wax.T, dtanh)         # (n_x, m)\n",
    "    dWax   = np.dot(dtanh, xt.T)          # (n_a, n_x)\n",
    "    da_prev = np.dot(Waa.T, dtanh)        # (n_a, m)\n",
    "    dWaa   = np.dot(dtanh, a_prev.T)      # (n_a, n_a)\n",
    "    dba    = np.sum(dtanh, axis=1, keepdims=True)  # (n_a, 1)\n",
    "\n",
    "    gradients = {\n",
    "        \"dxt\": dxt,\n",
    "        \"da_prev\": da_prev,\n",
    "        \"dWax\": dWax,\n",
    "        \"dWaa\": dWaa,\n",
    "        \"dba\": dba\n",
    "    }\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(da, caches):\n",
    "    \"\"\"\n",
    "    Implements the backward pass for a vanilla RNN over an entire sequence.\n",
    "\n",
    "    Arguments:\n",
    "    da -- Gradients of all hidden states, shape (n_a, m, T_x)\n",
    "    caches -- Tuple of values from rnn_forward (list of caches, x)\n",
    "\n",
    "    Returns:\n",
    "    gradients -- Dictionary with:\n",
    "        dx -- Gradient of input data, shape (n_x, m, T_x)\n",
    "        da0 -- Gradient of initial hidden state, shape (n_a, m)\n",
    "        dWax -- Gradient of input weights, shape (n_a, n_x)\n",
    "        dWaa -- Gradient of hidden weights, shape (n_a, n_a)\n",
    "        dba -- Gradient of bias vector, shape (n_a, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches_list, x = caches\n",
    "    (a1, a0, xt, parameters) = caches_list[0]\n",
    "\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, _ = xt.shape\n",
    "\n",
    "    dx    = np.zeros((n_x, m, T_x))\n",
    "    dWax  = np.zeros((n_a, n_x))\n",
    "    dWaa  = np.zeros((n_a, n_a))\n",
    "    dba   = np.zeros((n_a, 1))\n",
    "    da0   = np.zeros((n_a, m))\n",
    "\n",
    "    da_next = np.zeros((n_a, m))\n",
    "\n",
    "    for t in reversed(range(T_x)):\n",
    "        da_current = da[:, :, t] + da_next\n",
    "        gradients = rnn_cell_backward(da_current, caches_list[t])\n",
    "\n",
    "        dx[:, :, t] = gradients[\"dxt\"]\n",
    "        dWax += gradients[\"dWax\"]\n",
    "        dWaa += gradients[\"dWaa\"]\n",
    "        dba  += gradients[\"dba\"]\n",
    "        da_next = gradients[\"da_prev\"]\n",
    "\n",
    "    da0 = da_next\n",
    "\n",
    "    gradients = {\n",
    "        \"dx\": dx,\n",
    "        \"da0\": da0,\n",
    "        \"dWax\": dWax,\n",
    "        \"dWaa\": dWaa,\n",
    "        \"dba\": dba\n",
    "    }\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    parameters['Wax'] = np.random.randn(n_a, n_x) * 0.01\n",
    "    parameters['Waa'] = np.random.randn(n_a, n_a) * 0.01\n",
    "    parameters['Wya'] = np.random.randn(n_y, n_a) * 0.01\n",
    "    parameters['ba'] = np.zeros((n_a, 1))\n",
    "    parameters['by'] = np.zeros((n_y, 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, lr):\n",
    "    parameters['Wax'] += -lr * gradients['dWax']\n",
    "    parameters['Waa'] += -lr * gradients['dWaa']\n",
    "    parameters['Wya'] += -lr * gradients['dWya']\n",
    "    parameters['ba']  += -lr * gradients['dba'] \n",
    "    parameters['by']  += -lr * gradients['dby']\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(gradients, max_value):\n",
    "    dWaa = gradients['dWaa']\n",
    "    dWax = gradients['dWax']\n",
    "    dWya = gradients['dWya']\n",
    "    dba =  gradients['dba']\n",
    "    dby = gradients['dby']\n",
    "    \n",
    "    for gradient in [dWax, dWaa, dWya, dba, dby]:\n",
    "        np.clip(gradient, -max_value, max_value, out = gradient)\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"dba\": dba, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters, char_to_ix, length=20, temperature=1.0, start_char=None):\n",
    "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[0]\n",
    "    \n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    indices = []\n",
    "    \n",
    "    if start_char is not None:\n",
    "        x[char_to_ix[start_char]] = 1\n",
    "        indices.append(char_to_ix[start_char])\n",
    "\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    idx = -1\n",
    "    counter = 0\n",
    "    newline_char = char_to_ix['\\n']\n",
    "    \n",
    "    while idx != newline_char and counter < length:\n",
    "        a = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, x) + ba)\n",
    "        z = np.dot(Wya, a) + by\n",
    "        \n",
    "        # Apply temperature before softmax\n",
    "        z = z / temperature\n",
    "        y = softmax(z)\n",
    "\n",
    "        # Random sampling with temperature-scaled probabilities\n",
    "        idx = np.random.choice(range(vocab_size), p=y.flatten())\n",
    "        \n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        indices.append(idx)\n",
    "        \n",
    "        a_prev = a\n",
    "        counter += 1\n",
    "\n",
    "    if counter == length:\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('dinos.txt', 'r')\n",
    "data = file.read().lower()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Chars: 27\n",
      "Total Names: 19909\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "data_size = len(data)\n",
    "\n",
    "print('Unique Chars:', vocab_size)\n",
    "print('Total Names:', data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = { ch:i for i, ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\naachenosaurus', 'aachenosaurus\\n'),\n",
       " ('\\naardonyx', 'aardonyx\\n'),\n",
       " ('\\nabdallahsaurus', 'abdallahsaurus\\n')]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataset(data):\n",
    "    \"\"\"\n",
    "    Creates dataset with one example per dinosaur name.\n",
    "    Each x_seq starts with '\\n' and ends before the final character,\n",
    "    each y_seq starts from the first char and ends at '\\n'.\n",
    "\n",
    "    Returns:\n",
    "    examples -- List of (x_seq, y_seq) pairs for each name\n",
    "    \"\"\"\n",
    "    names = data.strip().split('\\n')\n",
    "    examples = []\n",
    "\n",
    "    for name in names:\n",
    "        full_name = '\\n' + name.lower() + '\\n'  # Add start and end token\n",
    "        x_seq = full_name[:-1]\n",
    "        y_seq = full_name[1:]\n",
    "        examples.append((x_seq, y_seq))\n",
    "\n",
    "    return examples\n",
    "\n",
    "examples = create_dataset(data)\n",
    "examples[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_example(x_seq, y_seq, char_to_ix, vocab_size):\n",
    "    T_x = len(x_seq)\n",
    "    x = np.zeros((vocab_size, 1, T_x))\n",
    "    y = np.zeros((vocab_size, 1, T_x))\n",
    "    for t in range(T_x):\n",
    "        x[char_to_ix[x_seq[t]], 0, t] = 1\n",
    "        y[char_to_ix[y_seq[t]], 0, t] = 1\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dino_name_from_indices(indices, ix_to_char):\n",
    "    name = ''.join([ix_to_char[ix] for ix in indices])\n",
    "    return name.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "n_a = 64\n",
    "n_x = vocab_size\n",
    "n_y = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(n_a=n_a, n_x=n_x, n_y=n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.2963\n",
      "Sample: [22, 23, 17, 9, 5, 5, 11, 22, 23, 2, 24, 24, 14, 16, 3, 2, 0]\n",
      "Sample Name: Vwqieekvwbxxnpcb\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 1000, Loss: 2.9999\n",
      "Sample: [5, 14, 16, 1, 3, 23, 16, 8, 19, 18, 7, 16, 23, 7, 11, 19, 12, 26, 21, 3, 0]\n",
      "Sample Name: Enpacwphsrgpwgkslzuc\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 2000, Loss: 2.5650\n",
      "Sample: [16, 19, 25, 19, 12, 18, 21, 1, 21, 9, 9, 9, 22, 5, 0]\n",
      "Sample Name: Psyslruauiiive\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 3000, Loss: 2.5197\n",
      "Sample: [15, 18, 19, 1, 19, 15, 15, 18, 18, 9, 21, 0]\n",
      "Sample Name: Orsasoorriu\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 4000, Loss: 2.2748\n",
      "Sample: [20, 18, 21, 19, 20, 14, 1, 1, 19, 21, 0]\n",
      "Sample Name: Trustnaasu\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 5000, Loss: 2.7683\n",
      "Sample: [1, 15, 1, 9, 18, 21, 16, 21, 6, 1, 18, 18, 16, 21, 19, 0]\n",
      "Sample Name: Aoairupufarrpus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 6000, Loss: 1.9483\n",
      "Sample: [9, 5, 15, 14, 15, 14, 1, 21, 18, 15, 19, 0]\n",
      "Sample Name: Ieononauros\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 7000, Loss: 1.1072\n",
      "Sample: [16, 22, 5, 3, 8, 15, 9, 1, 20, 1, 20, 20, 18, 19, 18, 21, 19, 0]\n",
      "Sample Name: Pvechoiatattrsrus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 8000, Loss: 1.5305\n",
      "Sample: [5, 15, 8, 15, 16, 15, 14, 15, 5, 0]\n",
      "Sample Name: Eohoponoe\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 9000, Loss: 2.6622\n",
      "Sample: [20, 15, 2, 20, 1, 8, 15, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Tobtahosaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 10000, Loss: 2.2221\n",
      "Sample: [1, 18, 3, 15, 19, 1, 19, 1, 21, 19, 0]\n",
      "Sample Name: Arcosasaus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 11000, Loss: 1.5391\n",
      "Sample: [20, 15, 25, 1, 14, 1, 18, 15, 12, 18, 1, 19, 0]\n",
      "Sample Name: Toyanarolras\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 12000, Loss: 1.4416\n",
      "Sample: [15, 12, 5, 5, 21, 19, 1, 19, 1, 0]\n",
      "Sample Name: Oleeusasa\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 13000, Loss: 2.8774\n",
      "Sample: [18, 21, 18, 1, 16, 0]\n",
      "Sample Name: Rurap\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 14000, Loss: 3.1867\n",
      "Sample: [1, 20, 1, 4, 8, 15, 12, 1, 20, 9, 0]\n",
      "Sample Name: Atadholati\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 15000, Loss: 2.1772\n",
      "Sample: [12, 15, 4, 1, 12, 15, 19, 1, 21, 18, 21, 19, 21, 19, 1, 21, 18, 21, 14, 0, 0]\n",
      "Sample Name: Lodalosaurususaurun\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 16000, Loss: 1.3787\n",
      "Sample: [5, 14, 1, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Enasaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 17000, Loss: 1.9064\n",
      "Sample: [19, 10, 1, 12, 15, 15, 7, 3, 15, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Sjaloogcosaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 18000, Loss: 1.6961\n",
      "Sample: [3, 20, 1, 5, 18, 1, 21, 18, 21, 14, 0]\n",
      "Sample Name: Ctaeraurun\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 19000, Loss: 2.2160\n",
      "Sample: [1, 14, 9, 18, 18, 19, 1, 12, 18, 21, 19, 0]\n",
      "Sample Name: Anirrsalrus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 20000, Loss: 2.5484\n",
      "Sample: [8, 21, 1, 18, 8, 1, 4, 9, 25, 20, 13, 15, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Huarhadiytmosaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 21000, Loss: 1.4591\n",
      "Sample: [12, 15, 1, 20, 9, 19, 1, 21, 8, 21, 19, 0]\n",
      "Sample Name: Loatisauhus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 22000, Loss: 2.1714\n",
      "Sample: [18, 5, 18, 15, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Rerosaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 23000, Loss: 2.3919\n",
      "Sample: [15, 14, 11, 10, 15, 19, 1, 21, 19, 21, 19, 0]\n",
      "Sample Name: Onkjosausus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 24000, Loss: 1.0737\n",
      "Sample: [1, 4, 23, 24, 1, 3, 5, 14, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Adwxacensaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 25000, Loss: 1.6354\n",
      "Sample: [9, 12, 21, 20, 15, 4, 15, 19, 1, 21, 18, 0]\n",
      "Sample Name: Ilutodosaur\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 26000, Loss: 1.4110\n",
      "Sample: [5, 22, 21, 11, 15, 18, 1, 14, 18, 21, 19, 0]\n",
      "Sample Name: Evukoranrus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 27000, Loss: 1.7821\n",
      "Sample: [13, 9, 16, 1, 4, 15, 19, 1, 21, 18, 15, 19, 0]\n",
      "Sample Name: Mipadosauros\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 28000, Loss: 1.3527\n",
      "Sample: [18, 9, 9, 20, 15, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Riitosaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 29000, Loss: 1.3870\n",
      "Sample: [10, 5, 18, 15, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Jerosaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 30000, Loss: 1.4330\n",
      "Sample: [21, 20, 9, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Utisaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 31000, Loss: 1.2097\n",
      "Sample: [12, 9, 7, 7, 15, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Liggosaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 32000, Loss: 2.3105\n",
      "Sample: [15, 5, 18, 1, 16, 20, 15, 18, 1, 16, 8, 21, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Oeraptoraphusaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 33000, Loss: 2.1909\n",
      "Sample: [4, 5, 4, 19, 1, 9, 12, 21, 19, 0]\n",
      "Sample Name: Dedsailus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 34000, Loss: 2.0772\n",
      "Sample: [12, 1, 16, 8, 5, 14, 4, 8, 21, 19, 1, 21, 18, 15, 19, 0]\n",
      "Sample Name: Laphendhusauros\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 35000, Loss: 1.5885\n",
      "Sample: [8, 21, 14, 4, 17, 19, 1, 21, 18, 9, 19, 0]\n",
      "Sample Name: Hundqsauris\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 36000, Loss: 2.2115\n",
      "Sample: [12, 5, 15, 16, 19, 1, 21, 18, 15, 19, 0]\n",
      "Sample Name: Leopsauros\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 37000, Loss: 1.9902\n",
      "Sample: [26, 23, 2, 15, 3, 5, 12, 9, 20, 0]\n",
      "Sample Name: Zwbocelit\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 38000, Loss: 1.2951\n",
      "Sample: [1, 21, 3, 8, 15, 20, 1, 16, 8, 1, 16, 20, 1, 0]\n",
      "Sample Name: Auchotaphapta\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 39000, Loss: 1.3691\n",
      "Sample: [18, 25, 8, 15, 14, 15, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Ryhonosaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 40000, Loss: 2.2029\n",
      "Sample: [1, 14, 7, 15, 14, 15, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Angonosaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 41000, Loss: 2.2474\n",
      "Sample: [18, 15, 2, 11, 21, 14, 0]\n",
      "Sample Name: Robkun\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 42000, Loss: 1.4945\n",
      "Sample: [21, 1, 16, 5, 14, 5, 21, 13, 21, 18, 21, 19, 0]\n",
      "Sample Name: Uapeneumurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 43000, Loss: 2.1550\n",
      "Sample: [15, 18, 15, 3, 5, 21, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Oroceusaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 44000, Loss: 1.9339\n",
      "Sample: [20, 1, 13, 15, 19, 21, 19, 0]\n",
      "Sample Name: Tamosus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 45000, Loss: 1.1072\n",
      "Sample: [2, 5, 14, 15, 18, 5, 19, 20, 15, 19, 25, 21, 18, 21, 19, 0]\n",
      "Sample Name: Benorestosyurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 46000, Loss: 2.1526\n",
      "Sample: [12, 21, 18, 1, 20, 1, 14, 8, 9, 20, 12, 15, 13, 15, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Luratanhitlomosaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 47000, Loss: 1.2367\n",
      "Sample: [1, 14, 14, 9, 1, 11, 9, 1, 21, 19, 0]\n",
      "Sample Name: Anniakiaus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 48000, Loss: 1.7768\n",
      "Sample: [15, 19, 9, 11, 15, 19, 1, 21, 18, 21, 19, 0]\n",
      "Sample Name: Osikosaurus\n",
      "\n",
      "--------------------------------------------------\n",
      "Epoch 49000, Loss: 2.2281\n",
      "Sample: [9, 25, 18, 15, 13, 8, 1, 21, 20, 21, 19, 0]\n",
      "Sample Name: Iyromhautus\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50000):\n",
    "    # Sample one random training pair\n",
    "    idx = np.random.randint(0, len(examples))\n",
    "    x_seq, y_seq = examples[idx]\n",
    "    x, y = vectorize_example(x_seq, y_seq, char_to_ix, vocab_size)\n",
    "    T_x = x.shape[2]\n",
    "    \n",
    "    # forward pass\n",
    "    a0 = np.zeros((n_a, 1))\n",
    "    a, y_pred, caches = rnn_forward(x, a0, parameters)\n",
    "    \n",
    "    # gradients wrt output\n",
    "    dy = y_pred - y\n",
    "    da = np.zeros_like(a)\n",
    "    for t in range(T_x):\n",
    "        da[:, :, t] = np.dot(parameters[\"Wya\"].T, dy[:, :, t])\n",
    "        \n",
    "    # backward pass\n",
    "    gradients = rnn_backward(da, caches)\n",
    "    \n",
    "    # dWya, dby (output layer gradients)\n",
    "    dWya = np.zeros_like(parameters[\"Wya\"])\n",
    "    dby  = np.zeros_like(parameters[\"by\"])\n",
    "    \n",
    "    for t in range(T_x):\n",
    "        dWya += np.dot(dy[:, :, t], a[:, :, t].T)\n",
    "        dby  += np.sum(dy[:, :, t], axis=1, keepdims=True)\n",
    "\n",
    "    gradients[\"dWya\"] = dWya\n",
    "    gradients[\"dby\"]  = dby\n",
    "    \n",
    "    gradients = clip_gradients(gradients, max_value=5.0)\n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "    \n",
    "    loss = -np.sum(y * np.log(y_pred + 1e-8)) / T_x\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        losses.append(loss)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "#         sam_indices = sample(parameters, char_to_ix, length=20)\n",
    "        sam_indices = sample(parameters, char_to_ix, length=20, temperature=1.0)\n",
    "        print(\"Sample:\", sam_indices)\n",
    "        print(\"Sample Name:\", get_dino_name_from_indices(sam_indices, ix_to_char))\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C ----> Chuanosaurus\n",
      "\n",
      "V ----> Veratos\n",
      "\n",
      "C ----> Cratasaurus\n",
      "\n",
      "J ----> Juranosaurus\n",
      "\n",
      "T ----> Tanlosaurus\n",
      "\n",
      "W ----> Wianosaurus\n",
      "\n",
      "V ----> Vurichisaurus\n",
      "\n",
      "B ----> Borosaurus\n",
      "\n",
      "Y ----> Yropatos\n",
      "\n",
      "T ----> Touryratrus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    start_char = random.choice(list(char_to_ix.keys()))\n",
    "    if start_char == '\\n': continue\n",
    "    indices = sample(parameters, char_to_ix, length=30, temperature=0.5, start_char=start_char)\n",
    "    name = get_dino_name_from_indices(indices, ix_to_char)\n",
    "    print(f\"{start_char.upper()} ----> {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"generated_names.txt\", \"a\") as f:\n",
    "    for ch in \"abcdefghijklmnopqrstuvwxyz\":\n",
    "        if ch in char_to_ix:\n",
    "            indices = sample(parameters, char_to_ix, length=30, temperature=0.8, start_char=ch)\n",
    "            name = get_dino_name_from_indices(indices, ix_to_char)\n",
    "            f.write(f\"{ch.upper()} ----> {name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
